{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hexo • All posts by \"hadoop\" category",
    "description": "",
    "home_page_url": "https://blog.asmyun.com",
    "items": [
        {
            "id": "https://blog.asmyun.com/2020/12/29/hadoop/hadoop-install/",
            "url": "https://blog.asmyun.com/2020/12/29/hadoop/hadoop-install/",
            "title": "hadoop_install",
            "date_published": "2020-12-29T06:01:41.000Z",
            "content_html": "<ul>\n<li><a href=\"#hadoop%E4%BB%8B%E7%BB%8D\">hadoop 介绍</a></li>\n<li><a href=\"#%E8%AE%B0%E5%BD%95%E9%9B%86%E8%BF%9E%E6%8E%A5\">记录集连接</a></li>\n<li><a href=\"#%E8%AE%B0%E5%BD%95%E5%85%B3%E8%81%94\">记录关联</a></li>\n</ul>\n<ul>\n<li>\n<a href=\"#\">Post not found: kettle-install 安装kettle</a>\n</li>\n<li>\n<a href=\"#\">Post not found: kettle-one 编写第一个作业</a>\n</li>\n<li>\n<a href=\"#\">Post not found: kettle-control 组件介绍</a>\n</li>\n<li>\n<a href=\"#\">Post not found: kettle-demo kettle示例</a>\n</li>\n</ul>\n<h4 id=\"hadoop介绍\"><a class=\"markdownIt-Anchor\" href=\"#hadoop介绍\">#</a> hadoop 介绍</h4>\n<h4 id=\"hadoop下载\"><a class=\"markdownIt-Anchor\" href=\"#hadoop下载\">#</a> hadoop 下载</h4>\n<h4 id=\"hadoop安装\"><a class=\"markdownIt-Anchor\" href=\"#hadoop安装\">#</a> hadoop 安装</h4>\n<h4 id=\"hadoop配置\"><a class=\"markdownIt-Anchor\" href=\"#hadoop配置\">#</a> hadoop 配置</h4>\n<h4 id=\"hadoop异常解决\"><a class=\"markdownIt-Anchor\" href=\"#hadoop异常解决\">#</a> hadoop 异常解决</h4>\n<h5 id=\"error-but-there-is-no-hdfs_namenode_user-defined-aborting-operation\"><a class=\"markdownIt-Anchor\" href=\"#error-but-there-is-no-hdfs_namenode_user-defined-aborting-operation\">#</a> ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.</h5>\n<p>The root cause of this problem,</p>\n<p>hadoop install for different user and you start yarn service for different user. OR<br>\nin hadoop config’s <span class=\"exturl\" data-url=\"aHR0cDovL2hhZG9vcC1lbnYuc2g=\">hadoop-env.sh</span> specified HDFS_NAMENODE_USER and HDFS_DATANODE_USER user is something else.<br>\nHence we need to correct and make it consistent at every place. So a simple solution of this problem is to edit your <span class=\"exturl\" data-url=\"aHR0cDovL2hhZG9vcC1lbnYuc2g=\">hadoop-env.sh</span> file and add the user-name for which you want to start the yarn service. So go ahead and edit $HADOOP_HOME/etc/hadoop/hadoop-env.sh by adding the following lines</p>\n<figure class=\"highlight shell\"><figcaption data-lang=\"Bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">HDFS_NAMENODE_USER</span><span class=\"token operator\">=</span>root</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">HDFS_DATANODE_USER</span><span class=\"token operator\">=</span>root</pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">HDFS_SECONDARYNAMENODE_USER</span><span class=\"token operator\">=</span>root</pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">YARN_RESOURCEMANAGER_USER</span><span class=\"token operator\">=</span>root</pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token builtin class-name\">export</span> <span class=\"token assign-left variable\">YARN_NODEMANAGER_USER</span><span class=\"token operator\">=</span>root</pre></td></tr></table></figure><h5 id=\"error-java_home-is-not-set-and-could-not-be-found\"><a class=\"markdownIt-Anchor\" href=\"#error-java_home-is-not-set-and-could-not-be-found\">#</a> ERROR: JAVA_HOME is not set and could not be found.</h5>\n<p>没有配置好 java 环境变量<br>\n第一种临时修改环境变量<br>\n <code>export JAVA_HOME=/usr/java/jdk1.6.0_45</code> <br>\n 第二种修改 hadoop JAVA_HOME<br>\n <code>cd $HADOOP_HOME/etc/hadoop</code> <br>\n <code>vim hadoop-env.sh</code></p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token comment\"># Many of the options here are built from the perspective that users</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># may want to provide OVERWRITING values on the command line.</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># For example:</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\">#</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token comment\">#  JAVA_HOME=/usr/java/testing hdfs dfs -ls</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\">#</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token comment\"># Therefore, the vast majority (BUT NOT ALL!) of these defaults</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token comment\"># are configured for substitution and not append.  If append</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token comment\"># is preferable, modify this file accordingly.</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre></pre></td></tr><tr><td data-num=\"11\"></td><td><pre><span class=\"token comment\">###</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre><span class=\"token comment\"># Generic settings for HADOOP</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre><span class=\"token comment\">###</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre></pre></td></tr><tr><td data-num=\"15\"></td><td><pre><span class=\"token comment\"># Technically, the only required environment variable is JAVA_HOME.</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre><span class=\"token comment\"># All others are optional.  However, the defaults are probably not</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre><span class=\"token comment\"># preferred.  Many sites configure these options outside of Hadoop,</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre><span class=\"token comment\"># such as in /etc/profile.d</span></pre></td></tr><tr><td data-num=\"19\"></td><td><pre></pre></td></tr><tr><td data-num=\"20\"></td><td><pre><span class=\"token comment\"># The java implementation to use. By default, this environment</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre><span class=\"token comment\"># variable is REQUIRED on ALL platforms except OS X!</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre>export JAVA_HOME<span class=\"token operator\">=</span><span class=\"token operator\">/</span>root<span class=\"token operator\">/</span>java<span class=\"token operator\">/</span>jdk1<span class=\"token punctuation\">.</span><span class=\"token number\">8.</span>0_271</pre></td></tr><tr><td data-num=\"23\"></td><td><pre></pre></td></tr><tr><td data-num=\"24\"></td><td><pre><span class=\"token comment\"># Location of Hadoop.  By default, Hadoop will attempt to determine</span></pre></td></tr><tr><td data-num=\"25\"></td><td><pre><span class=\"token comment\"># this location based upon its execution path.</span></pre></td></tr><tr><td data-num=\"26\"></td><td><pre><span class=\"token comment\"># export HADOOP_HOME=</span></pre></td></tr><tr><td data-num=\"27\"></td><td><pre></pre></td></tr><tr><td data-num=\"28\"></td><td><pre><span class=\"token comment\"># Location of Hadoop's configuration information.  i.e., where this</span></pre></td></tr><tr><td data-num=\"29\"></td><td><pre><span class=\"token comment\"># file is living. If this is not defined, Hadoop will attempt to</span></pre></td></tr><tr><td data-num=\"30\"></td><td><pre><span class=\"token comment\"># locate it based upon its execution path.</span></pre></td></tr><tr><td data-num=\"31\"></td><td><pre><span class=\"token comment\">#</span></pre></td></tr><tr><td data-num=\"32\"></td><td><pre><span class=\"token comment\"># NOTE: It is recommend that this variable not be set here but in</span></pre></td></tr><tr><td data-num=\"33\"></td><td><pre><span class=\"token comment\"># /etc/profile.d or equivalent.  Some options (such as</span></pre></td></tr><tr><td data-num=\"34\"></td><td><pre><span class=\"token comment\"># --config) may react strangely otherwise.</span></pre></td></tr><tr><td data-num=\"35\"></td><td><pre><span class=\"token comment\">#</span></pre></td></tr><tr><td data-num=\"36\"></td><td><pre><span class=\"token comment\"># export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span></pre></td></tr><tr><td data-num=\"37\"></td><td><pre></pre></td></tr><tr><td data-num=\"38\"></td><td><pre><span class=\"token comment\"># The maximum amount of heap to use (Java -Xmx).  If no unit</span></pre></td></tr><tr><td data-num=\"39\"></td><td><pre><span class=\"token comment\"># is provided, it will be converted to MB.  Daemons will</span></pre></td></tr><tr><td data-num=\"40\"></td><td><pre><span class=\"token comment\"># prefer any Xmx setting in their respective _OPT variable.</span></pre></td></tr><tr><td data-num=\"41\"></td><td><pre><span class=\"token comment\"># There is no default; the JVM will autoscale based upon machine</span></pre></td></tr><tr><td data-num=\"42\"></td><td><pre><span class=\"token comment\"># memory size.</span></pre></td></tr><tr><td data-num=\"43\"></td><td><pre><span class=\"token comment\"># export HADOOP_HEAPSIZE_MAX=</span></pre></td></tr></table></figure><h5 id=\"permission-denied-publickeygssapi-keyexgssapi-with-micpassword\"><a class=\"markdownIt-Anchor\" href=\"#permission-denied-publickeygssapi-keyexgssapi-with-micpassword\">#</a> Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).</h5>\n",
            "tags": []
        }
    ]
}